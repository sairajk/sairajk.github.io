<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Perla Sai Raj Kishore</title>
  
  <meta name="author" content="Perla Sai Raj Kishore">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Perla Sai Raj Kishore</name>
                <br>
                <font size="3"><br/>Learning to teach machines how to learn.</font>
              </p>
              <br/>
              <p align="justify"><font size="3">
              I am a Master's student at the <a href="https://www.sfu.ca/computing.html" target="_blank"><font size="3">School of Computing Science</font></a>, Simon Fraser University (SFU).
              </font></p>
              <p align="justify"><font size="3">
              Before joining SFU, I worked as a Senior Research Engineer at <a href="https://www.staqu.com/" target="_blank"><font size="3">Staqu Technologies</font></a>, where I designed and developed systems that revolved around Computer Vision and Deep Learning. And even earlier, during my bachelor's, I worked with <a href="https://www.isical.ac.in/~ujjwal/" target="_blank"><font size="3">Prof. Ujjwal Bhattacharya</font></a> of <a href="https://www.isical.ac.in/" target="_blank"><font size="3">Indian Statistical Institute (ISI), Kolkata</font></a> and <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html" target="_blank"><font size="3">Prof. Partha Pratim Roy</font></a> of <a href="https://www.iitr.ac.in" target="_blank"><font size="3">Indian Institute of Technology (IIT) Roorkee</font></a> on a variety of Computer Vision research problems.
              </font></p>
              <br>
              <p style="text-align:center">
                <a href="mailto:sairajkishore13@gmail.com" target="_blank"><font size="3">Email</font></a> &nbsp|&nbsp
                <a href="https://drive.google.com/file/d/1dwvJ5dMnrE5z3u0MLt-1QdyNL8RfEPQ6/view?usp=sharing" target="_blank">CV</a> &nbsp|&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp|&nbsp -->
                <a href="https://scholar.google.com/citations?user=uYP3cIcAAAAJ&hl=en&authuser=2" target="_blank"><font size="3">Google Scholar</font></a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/sairajkishore/" target="_blank"><font size="3"> LinkedIn </font></a>  &nbsp|&nbsp
                <a href="https://github.com/sairajk" target="_blank"><font size="3"> GitHub </font></a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar_2.png" target="_blank"><img style="width:100%;max-width:100%" alt="Profile Photo" src="images/avatar_2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- RESEARCH -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
              <heading><font size="5">Research Interests</font></heading>
              <p><font size="3">
                I am interested in Computer Vision, Computer Graphics, and Deep Learning in general. Particularly, understanding the world around from 2D & 3D visual data through systems that can effectively utilize the acquired knowledge and data from other similar tasks and domains, learn from data with limited or no labels, and are robust in diverse real-world scenarios.
              </font></p>
            </td>
          </tr>
        </tbody></table>

        <!-- PUBLICATIONS -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading><font size="5">Publications</font></heading>
            </td>
          </tr>
        </tbody></table>

        <script>
          function myFunction(pub_name) {
              var x = document.getElementById(pub_name);
              if (x.style.display === 'none') {
                  x.style.display = 'block';
              } else {
                  x.style.display = 'none';
              }
        }
        </script>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



          
          <!-- UNDER REVIEW - POSE ESTIMATION -->
          <!-- <tr onmouseout="underreview_0_stop()" onmouseover="underreview_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='underreview_0_image'><img src='images/Pub-ur_pose_re.jpg'></div>
                <img src='images/Pub-ur_pose_re.jpg'>
              </div>
              <script type="text/javascript">
                function underreview_0_start() {
                  document.getElementById('underreview_0_image').style.opacity = "1";
                }

                function underreview_0_stop() {
                  document.getElementById('underreview_0_image').style.opacity = "0";
                }
                underreview_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="TODO" target="_blank">
                <papertitle><font size="3">An End-to-End Framework for Unsupervised Pose Estimation of Occluded Pedestrians</font></papertitle>
              </a>
              <br>
              <font size="3">
              <a href="https://sudip.info/" target="_blank"><font size="3">Sudip Das*</font></a>, 
              <strong><font size="3">Perla Sai Raj Kishore*</font></strong>,
              <a href="https://www.isical.ac.in/~ujjwal/" target="_blank"><font size="3">Ujjwal Bhattacharya</font></a>
              <br>
              <em>Under Review</em>, 2020
              </font>
              <br>
              <p></p>              
            </td>
          </tr> -->


          <!-- ICIP 2020 - POSE ESTIMATION -->
          <tr onmouseout="icip20_0_stop()" onmouseover="icip20_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icip20_0_image'><img src='images/Pub-icip20_pose_re.jpg'></div>
                <img src='images/Pub-icip20_pose_re.jpg'>
              </div>
              <script type="text/javascript">
                function icip20_0_start() {
                  document.getElementById('icip20_0_image').style.opacity = "1";
                }

                function icip20_0_stop() {
                  document.getElementById('icip20_0_image').style.opacity = "0";
                }
                icip20_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9191147" target="_blank">
                <papertitle><font size="3">An End-To-End Framework For Pose Estimation of Occluded Pedestrians</font></papertitle>
              </a>
              <br>
              <font size="3">
              <a href="https://sudip.info/" target="_blank"><font size="3">Sudip Das*</font></a>, 
              <strong><font size="3">Perla Sai Raj Kishore*</font></strong>,
              <a href="https://www.isical.ac.in/~ujjwal/" target="_blank"><font size="3">Ujjwal Bhattacharya</font></a>
              <br>
              <em>International Conference on Image Processing (ICIP)</em>, 2020
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('icip20_0_abs')"><font size="3">Abstract</font></a> /
              <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
              <a href="javascript:void(0);" onclick="myFunction('icip20_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="icip20_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Pose estimation in the wild is a challenging problem, particularly in situations of(i) occlusions of varying degrees, and (ii) crowded outdoor scenes. Most of the existing studies of pose estimation did not report the performance in similar situations. Moreover, pose annotations for occluded parts of the human figures have not been provided in any of the relevant standard datasets, which in turn creates further difficulties to the required studies for pose estimation of the entire Figure for occluded humans. Well known pedestrian detection datasets such as CityPersons contains samples of outdoor scenes but it does not include pose annotations. Here we propose a novel multi-task framework for end-to-end training towards the entire pose estimation of pedestrians including in situations of any kind of occlusion. To tackle this problem, we make use of a pose estimation dataset, MS-COCO, and employ unsupervised adversarial instance-level domain adaptation for estimating the entire pose of occluded pedestrians. The experimental studies show that the proposed framework outperforms the SOTA results for pose estimation, instance segmentation and pedestrian detection in cases of heavy occlusions (HO) and reasonable + heavy occlusions (R+HO) on the two benchmark datasets.
                </em>
              </font></div>
              <div id="icip20_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @INPROCEEDINGS{9191147,<br>
                  &emsp;author={S. {Das} and P. S. R. {Kishore} and U. {Bhattacharya}},<br>
                  &emsp;booktitle={2020 IEEE International Conference on Image Processing (ICIP)},<br> 
                  &emsp;title={An End-To-End Framework For Pose Estimation Of Occluded Pedestrians},<br> 
                  &emsp;year={2020},<br>
                  &emsp;volume={},<br>
                  &emsp;number={},<br>
                  &emsp;pages={1446-1450},<br>
                  &emsp;doi={10.1109/ICIP40778.2020.9191147}<br>
                }
              </font></div>
            </td>
          </tr>





          <!-- BMVC 2019 - POSE ESTIMATION -->
          <tr onmouseout="bmvc19_0_stop()" onmouseover="bmvc19_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bmvc19_0_image'><img src='images/Pub-bmvc19_pose_re.jpg'></div>
                <img src='images/Pub-bmvc19_pose_re.jpg'>
              </div>
              <script type="text/javascript">
                function bmvc19_0_start() {
                  document.getElementById('bmvc19_0_image').style.opacity = "1";
                }

                function bmvc19_0_stop() {
                  document.getElementById('bmvc19_0_image').style.opacity = "0";
                }
                bmvc19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://bmvc2019.org/wp-content/uploads/papers/0466-paper.pdf" target="_blank">
                <papertitle><font size="3">ClueNet: A Deep Framework for Occluded Pedestrian Pose Estimation</font></papertitle>
              </a>
              <br>
              <font size="3">
              <strong><font size="3">Perla Sai Raj Kishore*</font></strong>,
              <a href="https://sudip.info/" target="_blank"><font size="3">Sudip Das*</font></a>,
              Partha Sarathi Mukherjee,
              <a href="https://www.isical.ac.in/~ujjwal/" target="_blank"><font size="3">Ujjwal Bhattacharya</font></a>
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2019
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('bmvc19_0_abs')"><font size="3">Abstract</font></a> /
              <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
              <a href="javascript:void(0);" onclick="myFunction('bmvc19_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="bmvc19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Pose estimation of a pedestrian helps to gather information about the current activity or the instant behaviour of the subject. Such information is useful for autonomous vehicles, augmented reality, video surveillance, etc. Although a large volume of pedestrian detection studies are available in the literature, detection of the same in situations of significant occlusions still remains a challenging task. In this work, we take a step further to propose a novel deep learning framework, called ClueNet, to detect as well as estimate the entire pose of occluded pedestrians in an unsupervised manner. ClueNet is a two stage framework where the first stage generates visual clues for the second stage to accurately estimate the pose of occluded pedestrians. The first stage employs a multi-task network to segment the visible parts and predict a bounding box enclosing the visible and occluded regions for each pedestrian. The second stage uses these predictions from the first stage for pose estimation. Here we propose a novel strategy, called Mask and Predict, to train our ClueNet to estimate the pose even for occluded regions. Additionally, we make use of various other training strategies to further improve our results. The proposed work is first of its kind and the experimental results on CityPersons and MS COCO datasets show the superior performance of our approach over existing methods.
                </em>
              </font></div>
              <div id="bmvc19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @article{kishore2019cluenet,<br>
                  &emsp;title={ClueNet: A Deep Framework for Occluded Pedestrian Pose Estimation},<br>
                  &emsp;author={Kishore, Perla Sai Raj and Das, Sudip and Mukherjee, Partha Sarathi and Bhattacharya, Ujjwal},<br>
                  &emsp;year={2019}<br>
                }
              </font></div>
            </td>
          </tr>


          <!-- CVPR 2019 - HANDWRITING RECOGNITION -->
          <tr onmouseout="cvpr19_0_stop()" onmouseover="cvpr19_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr19_0_image'><img src='images/Pub-cvpr19_hr_after_re.jpg'></div>
                <img src='images/Pub-cvpr19_hr_before_re.jpg'>
              </div>
              <script type="text/javascript">
                function cvpr19_0_start() {
                  document.getElementById('cvpr19_0_image').style.opacity = "1";
                }

                function cvpr19_0_stop() {
                  document.getElementById('cvpr19_0_image').style.opacity = "0";
                }
                cvpr19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Bhunia_Handwriting_Recognition_in_Low-Resource_Scripts_Using_Adversarial_Learning_CVPR_2019_paper.html" target="_blank">
                <papertitle><font size="3">Handwriting Recognition in Low-Resource Scripts Using Adversarial Learning</font></papertitle>
              </a>
              <br>
              <font size="3">
              <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank"><font size="3">Ayan Kumar Bhunia</font></a>,
              Abhirup Das,
              <a href="https://ankanbhunia.github.io/" target="_blank"><font size="3">Ankan Kumar Bhunia</font></a>,
              <strong><font size="3">Perla Sai Raj Kishore</font></strong>,
              <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html" target="_blank"><font size="3">Partha Pratim Roy</font></a>
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('cvpr19_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/AyanKumarBhunia/Handwriting_Recogition_using_Adversarial_Learning" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/1811.01396" target="_blank" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('cvpr19_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="cvpr19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Handwritten Word Recognition and Spotting is a challenging field dealing with handwritten text possessing irregular and complex shapes. The design of deep neural network models makes it necessary to extend training datasets in order to introduce variations and increase the number of samples; word-retrieval is therefore very difficult in low-resource scripts. Much of the existing literature comprises preprocessing strategies which are seldom sufficient to cover all possible variations. We propose an Adversarial Feature Deformation Module (AFDM) that learns ways to elastically warp extracted features in a scalable manner. The AFDM is inserted between intermediate layers and trained alternatively with the original framework, boosting its capability to better learn highly informative features rather than trivial ones. We test our meta-framework, which is built on top of popular word-spotting and word-recognition frameworks and enhanced by AFDM, not only on extensive Latin word datasets but also on sparser Indic scripts. We record results for varying sizes of training data, and observe that our enhanced network generalizes much better in the low-data regime; the overall word-error rates and mAP scores are observed to improve as well.
                </em>
              </font></div>
              <div id="cvpr19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @InProceedings{Bhunia_2019_CVPR,<br>
                  &emsp;author = {Bhunia, Ayan Kumar and Das, Abhirup and Bhunia, Ankan Kumar and Kishore, Perla Sai Raj and Roy, Partha Pratim},<br>
                  &emsp;title = {Handwriting Recognition in Low-Resource Scripts Using Adversarial Learning},<br>
                  &emsp;booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                  &emsp;month = {June},<br>
                  &emsp;year = {2019}<br>
                }
              </font></div>
            </td>
          </tr>


          <!-- ICASSP 2019 - THUMBNAIL GENERATION -->
          <tr onmouseout="icassp19_0_stop()" onmouseover="icassp19_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icassp19_0_image'><img src='images/Pub-icassp19_thumbnail_after_re.jpg'></div>
                <img src='images/Pub-icassp19_thumbnail_before_re.jpg'>
              </div>
              <script type="text/javascript">
                function icassp19_0_start() {
                  document.getElementById('icassp19_0_image').style.opacity = "1";
                }

                function icassp19_0_stop() {
                  document.getElementById('icassp19_0_image').style.opacity = "0";
                }
                icassp19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/8683761" target="_blank">
                <papertitle><font size="3">User Constrained Thumbnail Generation Using Adaptive Convolutions</font></papertitle>
              </a>
              <br>
              <font size="3">
              <strong><font size="3">Perla Sai Raj Kishore</font></strong>,
              <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank"><font size="3">Ayan Kumar Bhunia</font></a>,
              Shovozit Ghose,
              <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html" target="_blank"><font size="3">Partha Pratim Roy</font></a>
              <br>
              <em>International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2019
              <br>
              <font color="red" size="3"><strong>(Oral)</strong></font>
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('icassp19_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/sairajk/Thumbnail-Generation" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/1810.13054" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('icassp19_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="icassp19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Thumbnails are widely used all over the world as a preview for digital images. In this work we propose a deep neural framework to generate thumbnails of any size and aspect ratio, even for unseen values during training, with high accuracy and precision. We use Global Context Aggregation (GCA) and a modified Region Proposal Network (RPN) with adaptive convolutions to generate thumbnails in real time. GCA is used to selectively attend and aggregate the global context information from the entire image while the RPN is used to generate candidate bounding boxes for the thumbnail image. Adaptive convolution eliminates the difficulty of generating thumbnails of various aspect ratios by using filter weights dynamically generated from the aspect ratio information. The experimental results indicate the superior performance of the proposed model 1 over existing state-of-the-art techniques. 
                </em>
              </font></div>
              <div id="icassp19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @inproceedings{kishore2019user,<br>
                  &emsp;title={User Constrained Thumbnail Generation Using Adaptive Convolutions},
                  &emsp;author={Kishore, Perla Sai Raj and Bhunia, Ayan Kumar and Ghose, Shuvozit and Roy, Partha Pratim},<br>
                  &emsp;booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},<br>
                  &emsp;pages={1677--1681},<br>
                  &emsp;year={2019},<br>
                  &emsp;organization={IEEE}<br>
                }
              </font></div>
            </td>
          </tr>


          <!-- WACV 2019 - TEXTURE PAPER -->
          <tr onmouseout="wacv19_0_stop()" onmouseover="wacv19_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='wacv19_0_image'><img src='images/Pub-wacv19_texret_re.jpg'></div>
                <img src='images/Pub-wacv19_texret_re.jpg'>
              </div>
              <script type="text/javascript">
                function wacv19_0_start() {
                  document.getElementById('wacv19_0_image').style.opacity = "1";
                }

                function wacv19_0_stop() {
                  document.getElementById('wacv19_0_image').style.opacity = "0";
                }
                wacv19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/8659343" target="_blank">
                <papertitle><font size="3">Texture Synthesis Guided Deep Hashing for Texture Image Retrieval</font></papertitle>
              </a>
              <br>
              <font size="3">
              <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank"><font size="3">Ayan Kumar Bhunia</font></a>,
              <strong><font size="3">Perla Sai Raj Kishore</font></strong>,
              <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank"><font size="3">Pranay Mukherjee</font></a>,
              Abhirup Das,
              <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html" target="_blank"><font size="3">Partha Pratim Roy</font></a>
              <br>
              <em>Winter Conference on Applications of Computer Vision (WACV)</em>, 2019
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('wacv19_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('wacv19_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="wacv19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  With the large scale explosion of images and videos over the internet, efficient hashing methods have been developed to facilitate memory and time efficient retrieval of similar images. However, none of the existing works use hashing to address texture image retrieval mostly because of the lack of sufficiently large texture image databases. Our work addresses this problem by developing a novel deep learning architecture that generates binary hash codes for input texture images. For this, we first pre-train a Texture Synthesis Network (TSN) which takes a texture patch as input and outputs an enlarged view of the texture by injecting newer texture content. Thus it signifies that the TSN encodes the learnt texture specific information in its intermediate layers. In the next stage, a second network gathers the multi-scale feature representations from the TSN’s intermediate layers using channel-wise attention, combines them in a progressive manner to a dense continuous representation which is finally converted into a binary hash code with the help of individual and pairwise label information. The new enlarged texture patches from the TSN also help in data augmentation to alleviate the problem of insufficient texture data and are used to train the second stage of the network. Experiments on three public texture image retrieval datasets indicate the superiority of our texture synthesis guided hashing approach over existing state-of-the-art methods.
                </em>
              </font></div>
              <div id="wacv19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @inproceedings{bhunia2019texture,<br>
                  &emsp;title={Texture synthesis guided deep hashing for texture image retrieval},<br>
                  &emsp;author={Bhunia, Ayan Kumar and Perla, Sai Raj Kishore and Mukherjee, Pranay and Das, Abhirup and Roy, Partha Pratim},<br>
                  &emsp;booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},<br>
                  &emsp;pages={609--618},<br>
                  &emsp;year={2019},<br>
                  &emsp;organization={IEEE}<br>
                }
              </font></div>
            </td>
          </tr>


          <!-- IJAIN 2018 - ACTIVATION FUNCTIONS -->
          <tr onmouseout="ijain18_0_stop()" onmouseover="ijain18_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ijain18_0_image'><img src='images/Pub-ijain18_actfn_re.jpg'></div>
                <img src='images/Pub-ijain18_actfn_re.jpg'>
              </div>
              <script type="text/javascript">
                function ijain18_0_start() {
                  document.getElementById('ijain18_0_image').style.opacity = "1";
                }

                function ijain18_0_stop() {
                  document.getElementById('ijain18_0_image').style.opacity = "0";
                }
                ijain18_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://ijain.org/index.php/IJAIN/article/view/249" target="_blank">
                <papertitle><font size="3">Flatten-T Swish: A thresholded ReLU-Swish-like Activation Function for Deep Learning</font></papertitle>
              </a>
              <br>
              <font size="3">
              Hock Hung Chieng,
              <a href="https://community.uthm.edu.my/nhaniza" target="_blank"><font size="3">Noorhaniza Wahid</font></a>,
              <a href="https://community.uthm.edu.my/ongp" target="_blank"><font size="3">Ong Pauline</font></a>,
              <strong><font size="3">Perla Sai Raj Kishore</font></strong>
              <br>
              <em>International Journal of Advances in Intelligent Informatics (IJAIN)</em>, 2018 &nbsp 
              <a href="https://drive.google.com/file/d/1SxlaL16XTFdAdUj4fcUHsaLKD6X1-LrU/view?usp=sharing" target="_blank"><font color="red" size="3">
              <br>
              <strong>(Best Paper Award)</strong></font></a>
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('ijain18_0_abs')"><font size="3">Abstract</font></a> /
              <a href="https://github.com/HI160029/Activation_Function" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/1812.06247" target="_blank"><font size="3">arXiv</font></a> /
              <a href="javascript:void(0);" onclick="myFunction('ijain18_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="ijain18_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Activation functions are essential for deep learning methods to learn and perform complex tasks such as image classification. Rectified Linear Unit (ReLU) has been widely used and become the default activation function across the deep learning community since 2012. Although ReLU has been popular, however, the hard zero property of the ReLU has heavily hindering the negative values from propagating through the network. Consequently, the deep neural network has not been benefited from the negative representations. In this work, an activation function called Flatten-T Swish (FTS) that leverage the benefit of the negative values is proposed. To verify its performance, this study evaluates FTS with ReLU and several recent activation functions. Each activation function is trained using MNIST dataset on five different deep fully connected neural networks (DFNNs) with depth vary from five to eight layers. For a fair evaluation, all DFNNs are using the same configuration settings. Based on the experimental results, FTS with a threshold value, T=-0.20 has the best overall performance. As compared with ReLU, FTS (T=-0.20) improves MNIST classification accuracy by 0.13%, 0.70%, 0.67%, 1.07% and 1.15% on wider 5 layers, slimmer 5 layers, 6 layers, 7 layers and 8 layers DFNNs respectively. Apart from this, the study also noticed that FTS converges twice as fast as ReLU. Although there are other existing activation functions are also evaluated, this study elects ReLU as the baseline activation function.
                </em>
              </font></div>
              <div id="ijain18_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @article{IJAIN249|to_array:0,<br>
                  &emsp;author = {Hock Chieng and Noorhaniza Wahid and Ong Pauline and Sai Raj Kishore Perla},<br>
                  &emsp;title = {Flatten-T Swish: a thresholded ReLU-Swish-like activation function for deep learning},<br>
                  &emsp;journal = {International Journal of Advances in Intelligent Informatics},<br>
                  &emsp;volume = {4}, number = {2},<br>
                  &emsp;year = {2018},<br>
                  &emsp;pages = {76--86}, <br>
                  &emsp;doi = {10.26555/ijain.v4i2.249},<br>
                  &emsp;url = {http://ijain.org/index.php/IJAIN/article/view/249}<br>
                }
              </font></div>
            </td>
          </tr>


          <!-- <tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/loss_after.jpg'></div>
                <img src='images/loss_before.jpg'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }

                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=1xpZ0fL9h1y9RfcTyPgVkxUrF3VwdkBvq">
                <papertitle>A General and Adaptive Robust Loss Function</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1701.03077">arxiv</a> /
              <a href="https://drive.google.com/open?id=1HNveL7xSNh6Ss7sxLK8Mw2L1Fc-rRhL4">supplement</a> /
              <a href="https://youtu.be/BmNKbnF69eY">video</a> /
              <a href="https://www.youtube.com/watch?v=4IInDT_S0ow&t=37m22s">talk</a> / 
              <a href="https://drive.google.com/file/d/1GzRYRIfLHvNLT_QwjHoBjHkBbs3Nbf0x/view?usp=sharing">slides</a> / 
              <a href="https://github.com/google-research/google-research/tree/master/robust_loss">tensorflow code</a> /
              <a href="https://github.com/jonbarron/robust_loss_pytorch">pytorch code</a> /
              <a href="data/BarronCVPR2019_reviews.txt">reviews</a> /
              <a href="data/BarronCVPR2019.bib">bibtex</a>
              <p></p>
              <p>A single robust loss function is a superset of many other common robust loss functions, and allows training to automatically adapt the robustness of its own loss.</p>
            </td>
          </tr> -->

        </tbody></table> 

        <!-- Projects -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <br><br>
              <heading><font size="5">Projects</font></heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <!-- PROJECT - SALIENCY DETECTION, CVPR19, PyTorch -->
          <tr onmouseout="proj_sod_stop()" onmouseover="proj_sod_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='proj_sod_image'><img src='images/Proj-SOD_after_re.jpg'></div>
                <img src='images/Proj-SOD_before_re.jpg'>
              </div>
              <script type="text/javascript">
                function proj_sod_start() {
                  document.getElementById('proj_sod_image').style.opacity = "1";
                }

                function proj_sod_stop() {
                  document.getElementById('proj_sod_image').style.opacity = "0";
                }
                proj_sod_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
              <a href="https://github.com/sairajk/PyTorch-Pyramid-Feature-Attention-Network-for-Saliency-Detection" target="_blank">
                <papertitle><font size="3">Saliency Detection: PyTorch implementation of a CVPR 2019 Publication</font></papertitle>
              </a>
              <br><br>
              <font size="3">
              PyTorch implementation of the paper "<i>Pyramid Feature Attention Network for Saliency Detection</i>", published at CVPR 2019.
              </font>
              <br>
              <p></p>
              <a href="https://github.com/sairajk/PyTorch-Pyramid-Feature-Attention-Network-for-Saliency-Detection" target="_blank"><font size="3">Code</font></a> /
              <a href="https://arxiv.org/abs/1903.00179" target="_blank"><font size="3">Paper</font></a>
              <p></p>
            </td>
          </tr>


          <!-- PROJECT - IMAGE SUPER-RESOLUTION APPLICATION -->
          <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='proj_sisr_image'><img src='images/Proj-sisr_after_re.jpg'></div>
                <img src='images/Proj-sisr_before_re.jpg'>
              </div>
              <script type="text/javascript">
                function proj_sisr_start() {
                  document.getElementById('proj_sisr_image').style.opacity = "1";
                }

                function proj_sisr_stop() {
                  document.getElementById('proj_sisr_image').style.opacity = "0";
                }
                proj_sisr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
              <a href="https://github.com/sairajk/Image-Super-Resolution-Application" target="_blank">
                <papertitle><font size="3">Single Image Super Resolution</font></papertitle>
              </a>
              <br><br>
              <font size="3">
              Image Super Resolution aims to increase the resolution of an image by generating pixels that interpolate best between a given low resolution and the required high resolution image. I built a deep learning based model for this purpose. A large amount of diverse data was also collected to train this model. The model was implemented using Keras in Python and comes with an easy to use graphical user interface. This was my project as an intern under <a href="https://www.iiitd.ac.in/subramanyam" target="_blank"><font size="3">Prof. A. V. Subramanyam</font></a> of <a href="https://www.iiitd.ac.in" target="_blank"><font size="3">IIIT, Delhi</font></a>.
              </font>
              <br>
              <p></p>
              <a href="https://github.com/sairajk/Image-Super-Resolution-Application" target="_blank"><font size="3">Code</font></a>
              <p></p>
            </td>
          </tr>


          <!-- PROJECT - MIXTURE DENSITY NETWORKS -->
          <tr onmouseout="proj_mdn_stop()" onmouseover="proj_mdn_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='proj_mdn_image'><img src='images/Proj-MDN_re.jpg'></div>
                <img src='images/Proj-MDN_re.jpg'>
              </div>
              <script type="text/javascript">
                function proj_mdn_start() {
                  document.getElementById('proj_mdn_image').style.opacity = "1";
                }

                function proj_mdn_stop() {
                  document.getElementById('proj_mdn_image').style.opacity = "0";
                }
                proj_mdn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
              <a href="https://github.com/sairajk/Mixture-Density-Networks" target="_blank">
                <papertitle><font size="3">Mixture Density Networks</font></papertitle>
              </a>
              <br><br>
              <font size="3">
              Mixture Density Networks (MDNs) are an interesting way to address multimodality (where the input and output hold a one-to-many relationship). In such scenarios, instead of directly predicting the output we model the probability distribution of the output as a weighed mixture of several Gaussians from which we sample the actual output. In this project, I implemented univariate and bivariate MDNs in Python using Tensorflow.
              </font>
              <br>
              <p></p>
              <a href="https://github.com/sairajk/Mixture-Density-Networks" target="_blank"><font size="3">Code</font></a> /
              <!-- <a href="http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/" target="_blank"><font size="3">Interesting Blog</font></a> / -->
              <a href="https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf" target="_blank"><font size="3">Original Paper</font></a>
              <p></p>
            </td>
          </tr>


          <!-- PROJECT - LANGUAGE MODEL -->
          <tr onmouseout="proj_lm_stop()" onmouseover="proj_lm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='proj_lm_image'><img src='images/Proj-lm_after_re.jpg'></div>
                <img src='images/Proj-lm_before_re.jpg'>
              </div>
              <script type="text/javascript">
                function proj_lm_start() {
                  document.getElementById('proj_lm_image').style.opacity = "1";
                }

                function proj_lm_stop() {
                  document.getElementById('proj_lm_image').style.opacity = "0";
                }
                proj_lm_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
              <a href="https://github.com/sairajk/Language-Models" target="_blank">
                <papertitle><font size="3">Character Level Language Model</font></papertitle>
              </a>
              <br><br>
              <font size="3">
              Auto-correct and auto-complete, which have now become a standard feature in almost all virtual keyboards, make use of a language model at its core. In this project, I built an LSTM based character-level language model that aims to predict the next character from a sequence of input characters. The code for this project was written in Python using Tensorflow.
              </font>
              <br>
              <p></p>
              <a href="https://github.com/sairajk/Language-Models" target="_blank"><font size="3">Code</font></a> 
              <p></p>
            </td>
          </tr>


          <!-- PROJECT - LANE DETECTION NFS U2 -->
          <tr onmouseout="proj_lanedet_stop()" onmouseover="proj_lanedet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='proj_lanedet_image'><img src='images/Proj-Lane_Det_re.jpg'></div>
                <img src='images/Proj-Lane_Det_re.jpg'>
              </div>
              <script type="text/javascript">
                function proj_lanedet_start() {
                  document.getElementById('proj_lanedet_image').style.opacity = "1";
                }

                function proj_lanedet_stop() {
                  document.getElementById('proj_lanedet_image').style.opacity = "0";
                }
                proj_lanedet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
              <a href="https://github.com/sairajk/Lane-Detection-in-NFS-Underground-2" target="_blank">
                <papertitle><font size="3">Lane Detection in <i>NFS: Underground 2</i></font></papertitle>
              </a>
              <br><br>
              <font size="3">
              Self Driving Cars are one of the fascinating technologies in this modern world. Though the entire process, from perceiving the surroundings to getting the car to move, is fairly complex, the first step usually begins with the detection of lanes that guide the vehicle on the road. In this project, I attempt to detect lanes in real-time in one of the popular games, "<i>NFS: Underground 2</i>", using OpenCV in Python.
              </font>
              <br>
              <p></p>
              <a href="https://github.com/sairajk/Lane-Detection-in-NFS-Underground-2" target="_blank"><font size="3">Code</font></a>
              <p></p>
            </td>
          </tr>


          <!-- PROJECT - ML ALGORITHMS -->
          <tr onmouseout="proj_mlalgo_stop()" onmouseover="proj_mlalgo_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='proj_mlalgo_image'><img src='images/Proj-MlAlgo_after_re.jpg'></div>
                <img src='images/Proj-MlAlgo_before_re.jpg'>
              </div>
              <script type="text/javascript">
                function proj_mlalgo_start() {
                  document.getElementById('proj_mlalgo_image').style.opacity = "1";
                }

                function proj_mlalgo_stop() {
                  document.getElementById('proj_mlalgo_image').style.opacity = "0";
                }
                proj_mlalgo_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
              <a href="https://github.com/sairajk/Machine-Learning-Algorithms" target="_blank">
                <papertitle><font size="3">Machine Learning Algorithms</font></papertitle>
              </a>
              <br><br>
              <font size="3">
              In this project, I implemented various Machine Learning algorithms from scratch in Python using only Numpy.
              </font>
              <br>
              <p></p>
              <a href="https://github.com/sairajk/Machine-Learning-Algorithms" target="_blank"><font size="3">Code</font></a> 
              <p></p>
            </td>
          </tr>

        </tbody></table>

        <!-- Footer - Template Credits -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template credits : 
                <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>


      </td>
    </tr>
  </table>
</body>

</html>
